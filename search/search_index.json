{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"XGP is a tool for performing symbolic regression oriented towards machine learning. It can be used for classification and regression tasks. Please refer to the \"How it works\" section for an in-depth introduction to symbolic regression. Symbolic regression is a heavy algorithm to run, hence a good implementation is paramount to make it usable in practice. XGP is written in Go, a compiled language which is both fast and easy to write readable code with. The downside is that Go is not very practical for common data wrangling tasks. XGP is thus available as a CLI tool and can be imported by other languages (such as Python) to facililate it's usage and integration into data science pipelines.","title":"Introduction"},{"location":"cli/","text":"Command-line interface (CLI) Installation First, install Go , set your GOPATH , and make sure $GOPATH/bin is on your PATH . Here are some additional ressources depending on your operating system: Linux Mac Windows Next, regardless of your OS, you can install the xgp CLI with the following command. go install github.com/MaxHalford/xgp/cmd/xgp If xgp --help runs without any errors then the installation was successful. If you encounter an error feel free to open an issue on GitHub . Usage Tip Apart from the following documentation you can also check out the command-line usage examples . Tip Run xgp <command> -h to get help with a command. For example xgp fit -h will display the help for the fit command. Training The fit command trains programs against a training dataset and saves the best one to a JSON file. The only required argument is a path to a CSV file which acts as the training dataset. The dataset is that it should contain only numerical data. Moreover the first row should contain column names. Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while xgp does not handle missing values. Once your dataset is ready, you can train XGP on it with the following command. >>> xgp fit train.csv This will evaluate and evolve many programs with default values before finally outputting the best obtained program to a JSON file. By default the JSON file is named program.json . The JSON file can then be used by the predict command to make predictions on another dataset. Info Whether the task is classification or regression is guessed from the loss metric parameter. The available loss metrics are listed here There are many parameters you can use; the details and default values and are specified in the training parameters section >>> xgp fit train.csv --loss mse --val mae --gens 64 --indis 256 --parsimony 0 .001 The following parameters are available with the fit command in addition to training parameters. Argument Description Default ignore Comma-separated list of columns to ignore output Path where to save the JSON representation of the best program program.json target Name of the target column in the training and validation datasets y val Path to a validation dataset that can be used to monitor out-of-bag performance If you use the val argument then the best model of each generation will be scored against the validation dataset. The resulting score is called the out-of-bag score because it is obtained by making predictions on a dataset that the model hasn't seen. Predicting Once you have produced a program with the fit command you can use it to make predictions on a dataset. The test set should have exactly the same format as the training set. Specifically the columns in the test set should be ordered in the same way they were in the training set . >>> xgp predict test.csv This will make predictions on test.csv and save them to a specificied path. The default path is y_pred.csv . The following arguments are available for the predict command. Argument Description Default keep Comma-separated list of columns to keep in the CSV output output Path to the CSV output y_pred.csv program Path to the program used to make predictions program.json target Name of the target column in the CSV output y Scoring If you don't want to save predictions and instead only want to evaluate a program then you can use the score command. The score command will open a program, make predictions against a given dataset, and output a prediction score. By default the scoring metric is the loss metric used for training. >>> xgp score test.csv The following arguments are available for the score command. Argument Description Default eval Evaluation metric Same as the loss metric used during training program Path to the program to score program.json target Name of the target column in the dataset y Visualization Because programs can be represented as trees, Graphviz can be used to visualize them. The todot command takes a program as input and outputs the Graphviz representation of the program. You can then copy/paste the output and use a service such as webgraphviz to obtain the visualization. By default the output will not be saved to a file but will however be displayed in the terminal. >>> xgp todot program.json You can also feed the todot command a formula instead of a JSON file. >>> xgp todot \"sum(X[13], 42)\" The following arguments are available for the todot command. Argument Description Default output Path to the DOT file output program.dot save Save to a DOT file or not False shell Output in the terminal or not True","title":"CLI usage"},{"location":"cli/#command-line-interface-cli","text":"","title":"Command-line interface (CLI)"},{"location":"cli/#installation","text":"First, install Go , set your GOPATH , and make sure $GOPATH/bin is on your PATH . Here are some additional ressources depending on your operating system: Linux Mac Windows Next, regardless of your OS, you can install the xgp CLI with the following command. go install github.com/MaxHalford/xgp/cmd/xgp If xgp --help runs without any errors then the installation was successful. If you encounter an error feel free to open an issue on GitHub .","title":"Installation"},{"location":"cli/#usage","text":"Tip Apart from the following documentation you can also check out the command-line usage examples . Tip Run xgp <command> -h to get help with a command. For example xgp fit -h will display the help for the fit command.","title":"Usage"},{"location":"cli/#training","text":"The fit command trains programs against a training dataset and saves the best one to a JSON file. The only required argument is a path to a CSV file which acts as the training dataset. The dataset is that it should contain only numerical data. Moreover the first row should contain column names. Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while xgp does not handle missing values. Once your dataset is ready, you can train XGP on it with the following command. >>> xgp fit train.csv This will evaluate and evolve many programs with default values before finally outputting the best obtained program to a JSON file. By default the JSON file is named program.json . The JSON file can then be used by the predict command to make predictions on another dataset. Info Whether the task is classification or regression is guessed from the loss metric parameter. The available loss metrics are listed here There are many parameters you can use; the details and default values and are specified in the training parameters section >>> xgp fit train.csv --loss mse --val mae --gens 64 --indis 256 --parsimony 0 .001 The following parameters are available with the fit command in addition to training parameters. Argument Description Default ignore Comma-separated list of columns to ignore output Path where to save the JSON representation of the best program program.json target Name of the target column in the training and validation datasets y val Path to a validation dataset that can be used to monitor out-of-bag performance If you use the val argument then the best model of each generation will be scored against the validation dataset. The resulting score is called the out-of-bag score because it is obtained by making predictions on a dataset that the model hasn't seen.","title":"Training"},{"location":"cli/#predicting","text":"Once you have produced a program with the fit command you can use it to make predictions on a dataset. The test set should have exactly the same format as the training set. Specifically the columns in the test set should be ordered in the same way they were in the training set . >>> xgp predict test.csv This will make predictions on test.csv and save them to a specificied path. The default path is y_pred.csv . The following arguments are available for the predict command. Argument Description Default keep Comma-separated list of columns to keep in the CSV output output Path to the CSV output y_pred.csv program Path to the program used to make predictions program.json target Name of the target column in the CSV output y","title":"Predicting"},{"location":"cli/#scoring","text":"If you don't want to save predictions and instead only want to evaluate a program then you can use the score command. The score command will open a program, make predictions against a given dataset, and output a prediction score. By default the scoring metric is the loss metric used for training. >>> xgp score test.csv The following arguments are available for the score command. Argument Description Default eval Evaluation metric Same as the loss metric used during training program Path to the program to score program.json target Name of the target column in the dataset y","title":"Scoring"},{"location":"cli/#visualization","text":"Because programs can be represented as trees, Graphviz can be used to visualize them. The todot command takes a program as input and outputs the Graphviz representation of the program. You can then copy/paste the output and use a service such as webgraphviz to obtain the visualization. By default the output will not be saved to a file but will however be displayed in the terminal. >>> xgp todot program.json You can also feed the todot command a formula instead of a JSON file. >>> xgp todot \"sum(X[13], 42)\" The following arguments are available for the todot command. Argument Description Default output Path to the DOT file output program.dot save Save to a DOT file or not False shell Output in the terminal or not True","title":"Visualization"},{"location":"go/","text":"Go usage Installation Once you have installed Go , you can install XGO like any other Go package. go get github.com/MaxHalford/xgp Usage Although the full API is available on godoc , only a subset of it is relevant if all you want to do is train a program on a dataset. Instantiation The core struct for learning in XGP is the GP . A GP encapsulates all the logic for generating, evaluating, and evolving programs. Although you can instantiate an GP directly, you can (and should) do it by instantiating a GPConfig and calling it's NewGP method. You can also use the NewDefaultGPConfig method to instantiate a GPConfig with the default values outlines in the training parameters section . Even if you don't want to use the default values, it's a good idea to use NewDefaultGPConfig and then to set the fields you want to modify afterwards. var config = NewDefaultGPConfig () config . LossMetric = metrics . Accuracy {} config . Individuals = 42 config . Funcs = \"cos,sin,exp\" var estimator = config . NewGP () The GPConfig struct fields exactly match the ones indicated in the training parameters section . Training Once you have an GP , you're ready to call to it's Fit method to train it on a dataset. Here is the signature of the Fit method: func ( est * GP ) Fit ( // Required arguments X [][] float64 , Y [] float64 , // Optional arguments (can safely be nil) W [] float64 , XVal [][] float64 , YVal [] float64 , WVal [] float64 , verbose bool , ) error Just like for the CLI, the only required arguments to the GP 's Fit method are a matrix of features X and a list of targets Y . W can be used to weight the samples in X during program evaluation, which is particularly useful for higher-level learning algorithms such as boosting . One important thing to notice is that X and XVal should be ordered column-wise ; that is X[0] should access the first column in the dataset, not the first row. Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while XGP does not handle missing values. Like the val_set argument in the CLI, XVal , YVal , and WVal can be used to track the performance of the best program on out-of-bag data. notifyEvery can be used to indicate at what frequency (in terms of genetic algorithm generations) progress should be displayed. You can extract the best obtained Program with the BestProgram method. var best = gp . BestProgram () Finally the Fit method returns an error which you should handle. Prediction Once the Fit method has been called, the Predict method can be used to make predictions given a set of features. func ( est GP ) Predict ( X [][] float64 , predictProba bool ) ([] float64 , error ) The columns in X should be ordered in the same way as in the training set. The proba argument can be used to indicate if probabilities should be returned in the case of classification.","title":"Go usage"},{"location":"go/#go-usage","text":"","title":"Go usage"},{"location":"go/#installation","text":"Once you have installed Go , you can install XGO like any other Go package. go get github.com/MaxHalford/xgp","title":"Installation"},{"location":"go/#usage","text":"Although the full API is available on godoc , only a subset of it is relevant if all you want to do is train a program on a dataset.","title":"Usage"},{"location":"go/#instantiation","text":"The core struct for learning in XGP is the GP . A GP encapsulates all the logic for generating, evaluating, and evolving programs. Although you can instantiate an GP directly, you can (and should) do it by instantiating a GPConfig and calling it's NewGP method. You can also use the NewDefaultGPConfig method to instantiate a GPConfig with the default values outlines in the training parameters section . Even if you don't want to use the default values, it's a good idea to use NewDefaultGPConfig and then to set the fields you want to modify afterwards. var config = NewDefaultGPConfig () config . LossMetric = metrics . Accuracy {} config . Individuals = 42 config . Funcs = \"cos,sin,exp\" var estimator = config . NewGP () The GPConfig struct fields exactly match the ones indicated in the training parameters section .","title":"Instantiation"},{"location":"go/#training","text":"Once you have an GP , you're ready to call to it's Fit method to train it on a dataset. Here is the signature of the Fit method: func ( est * GP ) Fit ( // Required arguments X [][] float64 , Y [] float64 , // Optional arguments (can safely be nil) W [] float64 , XVal [][] float64 , YVal [] float64 , WVal [] float64 , verbose bool , ) error Just like for the CLI, the only required arguments to the GP 's Fit method are a matrix of features X and a list of targets Y . W can be used to weight the samples in X during program evaluation, which is particularly useful for higher-level learning algorithms such as boosting . One important thing to notice is that X and XVal should be ordered column-wise ; that is X[0] should access the first column in the dataset, not the first row. Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while XGP does not handle missing values. Like the val_set argument in the CLI, XVal , YVal , and WVal can be used to track the performance of the best program on out-of-bag data. notifyEvery can be used to indicate at what frequency (in terms of genetic algorithm generations) progress should be displayed. You can extract the best obtained Program with the BestProgram method. var best = gp . BestProgram () Finally the Fit method returns an error which you should handle.","title":"Training"},{"location":"go/#prediction","text":"Once the Fit method has been called, the Predict method can be used to make predictions given a set of features. func ( est GP ) Predict ( X [][] float64 , predictProba bool ) ([] float64 , error ) The columns in X should be ordered in the same way as in the training set. The proba argument can be used to indicate if probabilities should be returned in the case of classification.","title":"Prediction"},{"location":"how-it-works/","text":"How it works A quick introduction to symbolic regression Info If you're looking for an in depth explanation to genetic programming (of which symbolic regression is a subset) check out A Field Guide to Genetic Programming . Symbolic regression is a subset of genetic programming. It is a supervised learning method where the structure of the model is not fixed. For example in linear regression the model is constrained to have a certain shape, e.g. y = w_0x_0 + w_1x_1 + b In symbolic regression the model can take many shapes. For example the following equation is valid in symbolic regression. y = cos(x_0) \\times log(sin(x_1)) \\times \\frac{3.14}{x_0} + 2.17 Anything goes . Symbolic regression is thus a very flexible approach. The idea is to run an optimisation procedure to look for the best structure and the best parameters. The obvious downside is that the search space becomes huge and unexplorable by standard gradient-based methods. A symbolic regression model is composed of three different kinds of so-called operators : Functions which are basic mathematical functions such as \\(cos\\) and \\(log\\), Constants which are simply floating point values such as 3.14 and 2.17, Variables which are features in a dataset such as \\(x_0\\) and \\(x_1\\). Each operator, regardless of it's type, has an arity which determines how many operands it takes. For example the multiplication operator has an arity of 2. All functions have an arity of 1 or more, whilst constants and variables have an arity of 0. Arity is needed internally to determine if an equation is legal or not. For example a function with an arity of 2 should have two operands. The idea is that an equation can be represented as a tree with each node being an operator and each branch an operand. For example the following tree represents the equation presented above. Info This diagram was generated with the CLI's todot command . The goal of symbolic regression is to find an optimal combination of operators. Not only do appropriate operators have to be chosen, they also have to associated in a good way. The search space is very complex and cannot be explored with gradient-based optimization techniques. Instead symbolic regression relies on evolutionary algorithms such as genetic algorithms (which is what XGP uses) to perform the optimization. Under the hood XGP uses eaopt . Info Symbolic regression is part of the larger family of genetic programming . Info Although they are very close and related, genetic programming and genetic algorithms are different things. In genetic programming combinations of operators are usually referred to as programs . Like other genetic programming methods, the idea with symbolic regression is to evolve said programs, hopefully by making them at solving the machine learning task as hand. The first element needed to perform evolutionary optimization is to define the fitness of our programs. In a supervised learning setting this is rather easy; indeed we simply have to \"run\" a program, get it's output, and compute a so-called loss metric by comparing the output to the ground truth. The nice thing is that we can use any loss metric, indeed it doesn't have to be differentiable (which is a requirement of gradient-based optimization). Once we can evaluate the fitness of a program, we can create a set of random programs and sort them according to their fitness. There are different ways to produce random programs. On the one hand we can generate program trees that have a fixed height; this is called full initialization . On the other hand we can generate trees that have variable branch heights; this is called as grow initialization . A popular method is to use both methods randomly, which is referred to as ramped half-and-half initialization . Given a set of programs and associated fitnesses, we can generate new programs by combining the ones in the current population. This is called crossover and is one of the basic patterns of evolution. The idea is to repeatidly sample two parents from the initial population and produce one (or two, it depends) new programs so as to obtain a new population. Normally, if the sampling is weighted by the fitness of each program, the new population will have a higher average fitness. There are different ways to crossover programs. The most basic one (and the only one implemented in XGP for now) is called subtree crossover , it consists in picking random nodes from two programs and swapping them. New programs can also be generated by sampling individual programs and modifying them. This is called mutation . Again, there are different kinds of mutation, of which three are implement in XGP. First of all point mutation can be used to modify each operator one-by-one. For example point mutation can be used for tuning the constants. Secondly, hoist mutation consists in replacing a branch by one of it's leaves. Finally subtree mutation picks a random node in a tree and replaces with a randomly generated tree. Once a new population has been generated, we can evaluate it and repeat the evolution process. At each so-called generation we record the best program and put it aside. Once the process has stopped, either because a set number of generations has occurred or because a desired performance has been reached then the best ever program can be used as the final model. Of course there is a huge element of randomness to symbolic regression. Whats more there are many choices that have to be made. What operators should one pick? How about what evolution strategy to use? The goal of XGP is to provide a solid framework to work with symbolic regression by providing both an efficient implementation and a high-level API. XGP implementation details The core of XGP is implemented in Go. Go is a good fit for genetic programming because it's concurrency features play nicely with embarrassingly parallel situations such as genetic algorithms. Moreover because the running time of symbolic regression grows exponentially with the number of programs, having a compiled implementation saves a lot of time. XGP's core code is organized in subpackages. The op package contains all the available operators and the tree structure necessary for building programs. Each operator satisfies the following interface: type Operator interface { Eval ( X [][] float64 ) [] float64 Arity () uint Operand ( i uint ) Operator SetOperand ( i uint , op Operator ) Operator Simplify () Operator Diff ( i uint ) Operator Name () string String () string } An Operator can \"evaluate\" a matrix and produce a output. The matrix is a set of features oriented column-wise (some would say Fortran style) to optimize the huge amount of columnar operations symbolic regression has to perform. An Operator also has an arity and a name. An Operator has as many child Operator s as it's arity. An Operator can also be simplified and differentiated. At first the Operator interface seems to require a hefty amount of methods, but in practice it only takes around 100 lines of code to implement. At a higher-level, a Program is what is used to do the actual learning; it has the following signature: type Program struct { Tree tree . Tree GP * GP } The GP gives the Program context about what it is it has to learn. The GP contains a LossMetric field with determines how to score each Program and if the task is classification or regression. The GP is also the global structure that organizes the programs and handles the learning process. If you want to use XGP with Go then you'll be working with the GP struct. However you shouldn't directly instantiate an GP ; instead you should use the GPConfig struct where you can speficify training parameters before calling the NewGP method. The metrics package is a completely independent package that contains implementations of machine learning metrics (such as accuracy and logarithmic loss). In theory it could be traded for another package if a Go standard appears. The meta package can be used to implement meta-learning algorithms such as gradient boosting . This is predominantly what makes XGP competitive. XGP has a few tricks up its sleeves (and more are coming): Tree simplication: because programs are randomly modified it can occur that some parts of the program can be simplified. For example the formula add(mul(2, 3), 4) can simply be replaced by 10 . In practice catching these simplifications and avoiding unnecessary computations helps a lot. Bloat control: bloat is an unavoidable problem in genetic program. As the generations go on the programs will have a tendency to grow in complexity. First of all this increases the running time. It also produces complex programs that tend to overfit. By default XGP uses a parsimony coefficient to penalize programs based on the number of operators they possess. Controlling bloat can be seen as a form of regularization . Constant optimisation: the constants of the best program are \"polished\" using CMA-ES . This usually takes a negligible amount of time and helps a lot in practice. Line search: the step size used in gradient boosting is tuned via line search .","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"","title":"How it works"},{"location":"how-it-works/#a-quick-introduction-to-symbolic-regression","text":"Info If you're looking for an in depth explanation to genetic programming (of which symbolic regression is a subset) check out A Field Guide to Genetic Programming . Symbolic regression is a subset of genetic programming. It is a supervised learning method where the structure of the model is not fixed. For example in linear regression the model is constrained to have a certain shape, e.g. y = w_0x_0 + w_1x_1 + b In symbolic regression the model can take many shapes. For example the following equation is valid in symbolic regression. y = cos(x_0) \\times log(sin(x_1)) \\times \\frac{3.14}{x_0} + 2.17 Anything goes . Symbolic regression is thus a very flexible approach. The idea is to run an optimisation procedure to look for the best structure and the best parameters. The obvious downside is that the search space becomes huge and unexplorable by standard gradient-based methods. A symbolic regression model is composed of three different kinds of so-called operators : Functions which are basic mathematical functions such as \\(cos\\) and \\(log\\), Constants which are simply floating point values such as 3.14 and 2.17, Variables which are features in a dataset such as \\(x_0\\) and \\(x_1\\). Each operator, regardless of it's type, has an arity which determines how many operands it takes. For example the multiplication operator has an arity of 2. All functions have an arity of 1 or more, whilst constants and variables have an arity of 0. Arity is needed internally to determine if an equation is legal or not. For example a function with an arity of 2 should have two operands. The idea is that an equation can be represented as a tree with each node being an operator and each branch an operand. For example the following tree represents the equation presented above. Info This diagram was generated with the CLI's todot command . The goal of symbolic regression is to find an optimal combination of operators. Not only do appropriate operators have to be chosen, they also have to associated in a good way. The search space is very complex and cannot be explored with gradient-based optimization techniques. Instead symbolic regression relies on evolutionary algorithms such as genetic algorithms (which is what XGP uses) to perform the optimization. Under the hood XGP uses eaopt . Info Symbolic regression is part of the larger family of genetic programming . Info Although they are very close and related, genetic programming and genetic algorithms are different things. In genetic programming combinations of operators are usually referred to as programs . Like other genetic programming methods, the idea with symbolic regression is to evolve said programs, hopefully by making them at solving the machine learning task as hand. The first element needed to perform evolutionary optimization is to define the fitness of our programs. In a supervised learning setting this is rather easy; indeed we simply have to \"run\" a program, get it's output, and compute a so-called loss metric by comparing the output to the ground truth. The nice thing is that we can use any loss metric, indeed it doesn't have to be differentiable (which is a requirement of gradient-based optimization). Once we can evaluate the fitness of a program, we can create a set of random programs and sort them according to their fitness. There are different ways to produce random programs. On the one hand we can generate program trees that have a fixed height; this is called full initialization . On the other hand we can generate trees that have variable branch heights; this is called as grow initialization . A popular method is to use both methods randomly, which is referred to as ramped half-and-half initialization . Given a set of programs and associated fitnesses, we can generate new programs by combining the ones in the current population. This is called crossover and is one of the basic patterns of evolution. The idea is to repeatidly sample two parents from the initial population and produce one (or two, it depends) new programs so as to obtain a new population. Normally, if the sampling is weighted by the fitness of each program, the new population will have a higher average fitness. There are different ways to crossover programs. The most basic one (and the only one implemented in XGP for now) is called subtree crossover , it consists in picking random nodes from two programs and swapping them. New programs can also be generated by sampling individual programs and modifying them. This is called mutation . Again, there are different kinds of mutation, of which three are implement in XGP. First of all point mutation can be used to modify each operator one-by-one. For example point mutation can be used for tuning the constants. Secondly, hoist mutation consists in replacing a branch by one of it's leaves. Finally subtree mutation picks a random node in a tree and replaces with a randomly generated tree. Once a new population has been generated, we can evaluate it and repeat the evolution process. At each so-called generation we record the best program and put it aside. Once the process has stopped, either because a set number of generations has occurred or because a desired performance has been reached then the best ever program can be used as the final model. Of course there is a huge element of randomness to symbolic regression. Whats more there are many choices that have to be made. What operators should one pick? How about what evolution strategy to use? The goal of XGP is to provide a solid framework to work with symbolic regression by providing both an efficient implementation and a high-level API.","title":"A quick introduction to symbolic regression"},{"location":"how-it-works/#xgp-implementation-details","text":"The core of XGP is implemented in Go. Go is a good fit for genetic programming because it's concurrency features play nicely with embarrassingly parallel situations such as genetic algorithms. Moreover because the running time of symbolic regression grows exponentially with the number of programs, having a compiled implementation saves a lot of time. XGP's core code is organized in subpackages. The op package contains all the available operators and the tree structure necessary for building programs. Each operator satisfies the following interface: type Operator interface { Eval ( X [][] float64 ) [] float64 Arity () uint Operand ( i uint ) Operator SetOperand ( i uint , op Operator ) Operator Simplify () Operator Diff ( i uint ) Operator Name () string String () string } An Operator can \"evaluate\" a matrix and produce a output. The matrix is a set of features oriented column-wise (some would say Fortran style) to optimize the huge amount of columnar operations symbolic regression has to perform. An Operator also has an arity and a name. An Operator has as many child Operator s as it's arity. An Operator can also be simplified and differentiated. At first the Operator interface seems to require a hefty amount of methods, but in practice it only takes around 100 lines of code to implement. At a higher-level, a Program is what is used to do the actual learning; it has the following signature: type Program struct { Tree tree . Tree GP * GP } The GP gives the Program context about what it is it has to learn. The GP contains a LossMetric field with determines how to score each Program and if the task is classification or regression. The GP is also the global structure that organizes the programs and handles the learning process. If you want to use XGP with Go then you'll be working with the GP struct. However you shouldn't directly instantiate an GP ; instead you should use the GPConfig struct where you can speficify training parameters before calling the NewGP method. The metrics package is a completely independent package that contains implementations of machine learning metrics (such as accuracy and logarithmic loss). In theory it could be traded for another package if a Go standard appears. The meta package can be used to implement meta-learning algorithms such as gradient boosting . This is predominantly what makes XGP competitive. XGP has a few tricks up its sleeves (and more are coming): Tree simplication: because programs are randomly modified it can occur that some parts of the program can be simplified. For example the formula add(mul(2, 3), 4) can simply be replaced by 10 . In practice catching these simplifications and avoiding unnecessary computations helps a lot. Bloat control: bloat is an unavoidable problem in genetic program. As the generations go on the programs will have a tendency to grow in complexity. First of all this increases the running time. It also produces complex programs that tend to overfit. By default XGP uses a parsimony coefficient to penalize programs based on the number of operators they possess. Controlling bloat can be seen as a form of regularization . Constant optimisation: the constants of the best program are \"polished\" using CMA-ES . This usually takes a negligible amount of time and helps a lot in practice. Line search: the step size used in gradient boosting is tuned via line search .","title":"XGP implementation details"},{"location":"python/","text":"Python usage Installation Since version 1.5, Go code can be imported from Python as a dynamic-link library (DLL). This is what is done in the XGP Python package . Using a wheel If you're using one of the following setups then you are in luck because a wheel is available. In other words you don't need to have Go and GCC installed. manylinux x86_64 Python 3.5 \u2705 Python 3.6 \u2705 You need to have the wheel package installed. >>> pip install wheel Then you can install the wheel from PyPI. >>> pip install xgp Compile it yourself To compile the DLL you will need to have Go and GCC installed. Once this is done simply run: >>> pip install --no-binary :all: xgp This uses setuptools-golang to pull the needed Go dependencies and compile the DLL. You can also build the DLL yourself with the following command. This is mostly for development purposes. >>> go build -buildmode = c-shared -o xgp.so xgp/xgp.go Usage The XGP Python package exposes a scikit-learn API so you can use it in the same way as you would any other scikit-learn compliant code. Check out the Python examples to get a general feel. There are two estimators that you can use depending on if you're doing classification or regression, namely XGPClassifier and XGPRegressor . Both inherit from XGPModel and their only difference is that they have a different default loss metric. What's more XGPClassifier has a predict_proba method. After training you'll be able to access the program_str_ to see the best program the estimator found. The following snippet shows a basic usage example of XGPClassifier . from sklearn import datasets from sklearn import metrics from sklearn import model_selection import xgp X , y = datasets . load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , random_state = 42 ) model = xgp . XGPClassifier ( flavor = 'vanilla' , loss_metric = 'logloss' , funcs = 'sum,sub,mul,div' , n_individuals = 500 , n_generations = 100 , random_state = 42 , parsimony_coefficient = 0.01 ) model . fit ( X_train , y_train , eval_set = ( X_test , y_test ), verbose = True ) metric = metrics . log_loss print ( 'Train log-loss: {:.5f}' . format ( metric ( y_train , model . predict_proba ( X_train )))) print ( 'Test log-loss: {:.5f}' . format ( metric ( y_test , model . predict_proba ( X_test )))) This gives the following output: Train log-loss: 0.217573 Test log-loss: 0.191963 The full list of parameters is available in the training parameters section . Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while XGP does not handle missing values.","title":"Python usage"},{"location":"python/#python-usage","text":"","title":"Python usage"},{"location":"python/#installation","text":"Since version 1.5, Go code can be imported from Python as a dynamic-link library (DLL). This is what is done in the XGP Python package .","title":"Installation"},{"location":"python/#using-a-wheel","text":"If you're using one of the following setups then you are in luck because a wheel is available. In other words you don't need to have Go and GCC installed. manylinux x86_64 Python 3.5 \u2705 Python 3.6 \u2705 You need to have the wheel package installed. >>> pip install wheel Then you can install the wheel from PyPI. >>> pip install xgp","title":"Using a wheel"},{"location":"python/#compile-it-yourself","text":"To compile the DLL you will need to have Go and GCC installed. Once this is done simply run: >>> pip install --no-binary :all: xgp This uses setuptools-golang to pull the needed Go dependencies and compile the DLL. You can also build the DLL yourself with the following command. This is mostly for development purposes. >>> go build -buildmode = c-shared -o xgp.so xgp/xgp.go","title":"Compile it yourself"},{"location":"python/#usage","text":"The XGP Python package exposes a scikit-learn API so you can use it in the same way as you would any other scikit-learn compliant code. Check out the Python examples to get a general feel. There are two estimators that you can use depending on if you're doing classification or regression, namely XGPClassifier and XGPRegressor . Both inherit from XGPModel and their only difference is that they have a different default loss metric. What's more XGPClassifier has a predict_proba method. After training you'll be able to access the program_str_ to see the best program the estimator found. The following snippet shows a basic usage example of XGPClassifier . from sklearn import datasets from sklearn import metrics from sklearn import model_selection import xgp X , y = datasets . load_breast_cancer ( return_X_y = True ) X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , y , random_state = 42 ) model = xgp . XGPClassifier ( flavor = 'vanilla' , loss_metric = 'logloss' , funcs = 'sum,sub,mul,div' , n_individuals = 500 , n_generations = 100 , random_state = 42 , parsimony_coefficient = 0.01 ) model . fit ( X_train , y_train , eval_set = ( X_test , y_test ), verbose = True ) metric = metrics . log_loss print ( 'Train log-loss: {:.5f}' . format ( metric ( y_train , model . predict_proba ( X_train )))) print ( 'Test log-loss: {:.5f}' . format ( metric ( y_test , model . predict_proba ( X_test )))) This gives the following output: Train log-loss: 0.217573 Test log-loss: 0.191963 The full list of parameters is available in the training parameters section . Warning For the while XGP does not handle categorical data. You should preemptively encode the categorical features in your dataset before feeding it to XGP. The recommended way is to use label encoding for ordinal data and one-hot encoding for non-ordinal data. Warning For the while XGP does not handle missing values.","title":"Usage"},{"location":"training-parameters/","text":"Training parameters Overview The following tables gives an overview of all the parameters that can be used for training XGP. The defaults are the same regardless of where you're using XGP from (please open an issue if you notice any descrepancies). The values indicated for Go are the ones that can be passed to a GPConfig struct. For Python some parameters have to be passed in the fit method. The most important parameter is called flavor . It determines what kind of model to use. It can take one of the following values: vanilla : trains a single genetic programming instance. boosting : trains a gradient boosting machine that uses genetic programming instances as weak learners. Genetic programming parameters Name CLI Go Python Default value Loss metric; is used to if the task is classification or regression loss LossMetricName loss_metric mae (for Python XGPClassifier defaults to logloss) Evaluation metric eval EvalMetricName eval_metric (in fit ) Same as loss metric Parsimony coefficient parsimony ParsimonyCoefficient parsimony_coeff 0.00001 Polish the best program polish PolishBest polish_best true Authorized functions funcs Funcs funcs sum,sub,mul,div Constant minimum const_min ConstMin const_min -5 Constant maximum const_max ConstMax const_max 5 Constant probability p_const PConst p_const 0.5 Full initialization probability p_full PFull p_full 0.5 Terminal probability p_leaf PLeaf p_leaf 0.3 Minimum height min_height MinHeight min_height 3 Maximum height max_height MaxHeight max_height 5 Because XGP doesn't require the loss metric to be differentiable you can use any loss metric available. If you don't specify an evaluation metric then it will default to using the loss metric. XGP uses ramped half-and-half initialization; the full initialization probability determines the probability of using full initialization and consequently the probability of using grow initialization. Genetic algorithm parameters Name CLI Go Python Default value Number of populations pops NPopulations n_populations 1 Number of individuals per population indis NIndividuals n_individuals 50 Number of generations gens NGenerations n_generations 30 Hoist mutation probability p_hoist_mut PHoistMutation p_hoist_mutation 0.1 Subtree mutation probability p_sub_mut PSubtreeMutation p_sub_tree_mutation 0.1 Point mutation probability p_point_mut PPointMutation p_point_mutation 0.1 Point mutation rate point_mut_rate PointMutationRate point_mutation_rate 0.3 Subtree crossover probability p_sub_cross PSubtreeCrossover p_sub_tree_crossover 0.5 Ensemble learning parameters Ensemble learning is done via the meta package . For Python and the CLI you can use the flavor parameter to switch regime. For Go you have to initialize the desired struct yourself with the appropriate method (for example initialize the GradientBoosting struct with the NewGradientBoosting method). Name CLI Go Python Default value Number of rounds rounds nRounds n_rounds 100 Number of early stopping rounds nEarlyStoppingRounds early_stopping n_early_stopping_rounds 5 Learning rate learning_rate learningRate learning_rate 0.08 Use line search line_search lineSearcher line_search \u2705 Row sampling row_sampling rowSampling | row_sampling` 1 Column sampling col_sampling colSampling | col_sampling` 1 Use best rounds use_best useBest use_best_rounds \u2705 Monitoring frequency monitor_every monitorEvery monitor_every 1 Other parameters Name CLI Go Python Default value Random number seed seed Seed seed Random Verbose verbose verbose verbose \u2705 Loss metrics Genetic programming directly minimises a loss metric. Because the optimization is done with a genetic algorithm the loss metric doesn't have to be differentiable. Whether the task is classification or regression is thus determined from the loss metric. This is similar to how XGBoost and LightGBM handle things. Each loss metric has a short name that you can use whether you are using the CLI, Go, or Python. You can also use these short names to evaluate the performance of the model. For example you might want to optimise the ROC AUC while also keeping track of the accuracy. Name Short name Task Logloss logloss Classification Accuracy accuracy Classification Precision precision Classification Recall recall Classification F1-score f1 Classification ROC AUC roc_auc Classification Mean absolute error mae Regression Mean squared error mse Regression Root mean squared error rmse Regression R2 r2 Regression Absolute Pearson correlation pearson Regression Operators The following table lists all the available operators. Regardless of from where it is being used from, functions should be passed to XGP by concatenating the short names of the functions with a comma. For example to use the natural logarithm and the multiplication use log,mul . Code-wise the operators are all located in the op subpackage, of which the goal is to provide fast implementations for each operator. For the while the only accelerations that exist are the ones for the sum and the division which use assembly implementations made available by gonum/floats . Name Arity Short name Go struct Absolute value 1 abs Abs Addition 2 add Add Cosine 1 cos Cos Division 2 div Div Inverse 1 inv Inv Maximum 2 max Max Minimum 2 min Min Multiplication 2 mul Mul Negative value 1 neg Neg Sine 1 sin Sin Square 2 square Square Subtraction 2 sub Sub Safe-division is used, meaning that if a denominator is 0 then the result will default to 1.","title":"Training parameters"},{"location":"training-parameters/#training-parameters","text":"","title":"Training parameters"},{"location":"training-parameters/#overview","text":"The following tables gives an overview of all the parameters that can be used for training XGP. The defaults are the same regardless of where you're using XGP from (please open an issue if you notice any descrepancies). The values indicated for Go are the ones that can be passed to a GPConfig struct. For Python some parameters have to be passed in the fit method. The most important parameter is called flavor . It determines what kind of model to use. It can take one of the following values: vanilla : trains a single genetic programming instance. boosting : trains a gradient boosting machine that uses genetic programming instances as weak learners.","title":"Overview"},{"location":"training-parameters/#genetic-programming-parameters","text":"Name CLI Go Python Default value Loss metric; is used to if the task is classification or regression loss LossMetricName loss_metric mae (for Python XGPClassifier defaults to logloss) Evaluation metric eval EvalMetricName eval_metric (in fit ) Same as loss metric Parsimony coefficient parsimony ParsimonyCoefficient parsimony_coeff 0.00001 Polish the best program polish PolishBest polish_best true Authorized functions funcs Funcs funcs sum,sub,mul,div Constant minimum const_min ConstMin const_min -5 Constant maximum const_max ConstMax const_max 5 Constant probability p_const PConst p_const 0.5 Full initialization probability p_full PFull p_full 0.5 Terminal probability p_leaf PLeaf p_leaf 0.3 Minimum height min_height MinHeight min_height 3 Maximum height max_height MaxHeight max_height 5 Because XGP doesn't require the loss metric to be differentiable you can use any loss metric available. If you don't specify an evaluation metric then it will default to using the loss metric. XGP uses ramped half-and-half initialization; the full initialization probability determines the probability of using full initialization and consequently the probability of using grow initialization.","title":"Genetic programming parameters"},{"location":"training-parameters/#genetic-algorithm-parameters","text":"Name CLI Go Python Default value Number of populations pops NPopulations n_populations 1 Number of individuals per population indis NIndividuals n_individuals 50 Number of generations gens NGenerations n_generations 30 Hoist mutation probability p_hoist_mut PHoistMutation p_hoist_mutation 0.1 Subtree mutation probability p_sub_mut PSubtreeMutation p_sub_tree_mutation 0.1 Point mutation probability p_point_mut PPointMutation p_point_mutation 0.1 Point mutation rate point_mut_rate PointMutationRate point_mutation_rate 0.3 Subtree crossover probability p_sub_cross PSubtreeCrossover p_sub_tree_crossover 0.5","title":"Genetic algorithm parameters"},{"location":"training-parameters/#ensemble-learning-parameters","text":"Ensemble learning is done via the meta package . For Python and the CLI you can use the flavor parameter to switch regime. For Go you have to initialize the desired struct yourself with the appropriate method (for example initialize the GradientBoosting struct with the NewGradientBoosting method). Name CLI Go Python Default value Number of rounds rounds nRounds n_rounds 100 Number of early stopping rounds nEarlyStoppingRounds early_stopping n_early_stopping_rounds 5 Learning rate learning_rate learningRate learning_rate 0.08 Use line search line_search lineSearcher line_search \u2705 Row sampling row_sampling rowSampling | row_sampling` 1 Column sampling col_sampling colSampling | col_sampling` 1 Use best rounds use_best useBest use_best_rounds \u2705 Monitoring frequency monitor_every monitorEvery monitor_every 1","title":"Ensemble learning parameters"},{"location":"training-parameters/#other-parameters","text":"Name CLI Go Python Default value Random number seed seed Seed seed Random Verbose verbose verbose verbose \u2705","title":"Other parameters"},{"location":"training-parameters/#loss-metrics","text":"Genetic programming directly minimises a loss metric. Because the optimization is done with a genetic algorithm the loss metric doesn't have to be differentiable. Whether the task is classification or regression is thus determined from the loss metric. This is similar to how XGBoost and LightGBM handle things. Each loss metric has a short name that you can use whether you are using the CLI, Go, or Python. You can also use these short names to evaluate the performance of the model. For example you might want to optimise the ROC AUC while also keeping track of the accuracy. Name Short name Task Logloss logloss Classification Accuracy accuracy Classification Precision precision Classification Recall recall Classification F1-score f1 Classification ROC AUC roc_auc Classification Mean absolute error mae Regression Mean squared error mse Regression Root mean squared error rmse Regression R2 r2 Regression Absolute Pearson correlation pearson Regression","title":"Loss metrics"},{"location":"training-parameters/#operators","text":"The following table lists all the available operators. Regardless of from where it is being used from, functions should be passed to XGP by concatenating the short names of the functions with a comma. For example to use the natural logarithm and the multiplication use log,mul . Code-wise the operators are all located in the op subpackage, of which the goal is to provide fast implementations for each operator. For the while the only accelerations that exist are the ones for the sum and the division which use assembly implementations made available by gonum/floats . Name Arity Short name Go struct Absolute value 1 abs Abs Addition 2 add Add Cosine 1 cos Cos Division 2 div Div Inverse 1 inv Inv Maximum 2 max Max Minimum 2 min Min Multiplication 2 mul Mul Negative value 1 neg Neg Sine 1 sin Sin Square 2 square Square Subtraction 2 sub Sub Safe-division is used, meaning that if a denominator is 0 then the result will default to 1.","title":"Operators"}]}